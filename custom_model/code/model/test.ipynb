{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0ebe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "from transformers import ViTModel\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93b4b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Cross-Attention Block\n",
    "# -----------------------\n",
    "class CrossAttentionBlock(nn.Module):\n",
    "    def __init__(self, dim_q, dim_k, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=dim_q, num_heads=num_heads, batch_first=True)\n",
    "        self.norm_q = nn.LayerNorm(dim_q)\n",
    "        self.norm_k = nn.LayerNorm(dim_k)\n",
    "\n",
    "    def forward(self, q, k, v, key_padding_mask=None):\n",
    "        q_norm, k_norm = self.norm_q(q), self.norm_k(k)\n",
    "        attn_out, attn_weights = self.attn(q_norm, k_norm, v, key_padding_mask=key_padding_mask)\n",
    "        return q + attn_out, attn_weights\n",
    "\n",
    "\n",
    "class AttentionPool(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch, seq_len, hidden_dim)\n",
    "        returns: pooled (batch, hidden_dim)\n",
    "        \"\"\"\n",
    "        weights = self.proj(x).squeeze(-1)  # (batch, seq_len)\n",
    "        weights = F.softmax(weights, dim=-1)  # attention over sequence\n",
    "        pooled = torch.bmm(weights.unsqueeze(1), x).squeeze(1)  # (batch, hidden_dim)\n",
    "        return pooled\n",
    "    \n",
    "# -----------------------\n",
    "# Symmetric Multimodal Classifier\n",
    "# -----------------------\n",
    "class SymmetricMultimodalClassifier(nn.Module):\n",
    "    def __init__(self, text_model='bert-base-uncased', image_model='google/vit-base-patch16-224',\n",
    "                 hidden_dim=768, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.text_encoder = BertModel.from_pretrained(text_model)\n",
    "        self.image_encoder = ViTModel.from_pretrained(image_model)\n",
    "\n",
    "        self.text_to_img = CrossAttentionBlock(hidden_dim, hidden_dim)\n",
    "        self.img_to_text = CrossAttentionBlock(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.text_pool = AttentionPool(hidden_dim)\n",
    "        self.img_pool  = AttentionPool(hidden_dim)\n",
    "\n",
    "        self.text_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.img_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, images):\n",
    "        # Encode\n",
    "        text_feat = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        img_feat = self.image_encoder(pixel_values=images).last_hidden_state\n",
    "\n",
    "        # --- Symmetric Cross-Attention ---\n",
    "        text_key_padding = (input_ids == tokenizer.pad_token_id)  # True for PAD\n",
    "\n",
    "        # text -> image cross-attention\n",
    "        text_cross, attn_t2i = self.text_to_img(text_feat, img_feat, img_feat)  # no mask needed for image\n",
    "        # image -> text cross-attention\n",
    "        img_cross, attn_i2t = self.img_to_text(img_feat, text_cross, text_cross,\n",
    "                                            key_padding_mask=text_key_padding)\n",
    "\n",
    "        # --- Mean pooling ---\n",
    "        # text_emb = text_cross.mean(dim=1)\n",
    "        # img_emb = img_cross.mean(dim=1)\n",
    "\n",
    "        text_emb = self.text_pool(text_cross)\n",
    "        img_emb = self.img_pool(img_cross)\n",
    "        \n",
    "        # --- Normalized embeddings for contrastive learning ---\n",
    "        text_emb_n = F.normalize(self.text_proj(text_emb), dim=-1)\n",
    "        img_emb_n = F.normalize(self.img_proj(img_emb), dim=-1)\n",
    "\n",
    "        # --- Classification logits (use cross-attended text embedding) ---\n",
    "        logits = self.classifier(text_emb)\n",
    "\n",
    "        return logits, text_emb_n, img_emb_n, attn_t2i, attn_i2t\n",
    "\n",
    "# -----------------------\n",
    "# Symmetric InfoNCE Contrastive Loss\n",
    "# -----------------------\n",
    "def contrastive_loss(text_emb, img_emb, temperature=0.05):\n",
    "    sim_matrix = torch.matmul(text_emb, img_emb.T) / temperature\n",
    "    labels = torch.arange(sim_matrix.size(0), device=sim_matrix.device)\n",
    "    loss_i = F.cross_entropy(sim_matrix, labels)\n",
    "    loss_t = F.cross_entropy(sim_matrix.T, labels)\n",
    "    return (loss_i + loss_t) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fc94f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalNewsDataset(Dataset):\n",
    "    def __init__(self, txt_file, image_dir, tokenizer, max_len=128, transform=None):\n",
    "        self.data = pd.read_csv(txt_file, sep='\\t')\n",
    "        self.image_dir = image_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def get_img_id(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        img_id = str(row['id'])\n",
    "        return img_id\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        text = str(row['clean_title'])\n",
    "        img_id = str(row['id'])\n",
    "        label = torch.tensor(int(row['2_way_label']), dtype=torch.long)\n",
    "        \n",
    "        img_path = os.path.join(self.image_dir, f\"{img_id}.jpg\")\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            image = Image.new(\"RGB\", (224, 224), (0, 0, 0))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            image = transforms.ToTensor()(image)\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True, \n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].squeeze(0)\n",
    "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
    "        \n",
    "        return input_ids, attention_mask, image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d916f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/repo/project_deepfake/project/custom_model/models/checkpoints/multimodal_model.pth\"\n",
    "tokenizer_path = \"/repo/project_deepfake/project/custom_model/models/checkpoints/tokenizer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc108786",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "dataset_path = '/repo/project_deepfake/project/fakeddit_dataset'\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dataset = MultimodalNewsDataset(\n",
    "    txt_file=dataset_path+'/text/text.txt',\n",
    "    image_dir=dataset_path+'/images',\n",
    "    tokenizer=tokenizer,\n",
    "    transform=transform_train \n",
    ")\n",
    "\n",
    "# Split indices\n",
    "train_idx, val_idx = train_test_split(\n",
    "    range(len(dataset)),\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=dataset.data['2_way_label']\n",
    ")\n",
    "\n",
    "train_set = Subset(dataset, train_idx)\n",
    "val_set   = Subset(dataset, val_idx)\n",
    "\n",
    "val_set.dataset.transform = transform_val  \n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9789126a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set.dataset.get_img_id(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1793f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(model_path, map_location=device)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d447485c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "def evaluate_model_full(model, dataloader, device='cuda'):\n",
    "    \"\"\"\n",
    "    Evaluate model on dataloader and return probabilities and standard metrics.\n",
    "    Metrics: Accuracy, AUC, F1, Precision, Recall, Confusion Matrix\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids, attention_mask, images, labels = batch\n",
    "\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(input_ids, attention_mask, images)\n",
    "            logits = output[0] if isinstance(output, tuple) else output  # (B,2)\n",
    "\n",
    "            # Probabilities for class 1\n",
    "            probs = torch.softmax(logits, dim=1)[:, 1]\n",
    "\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    all_probs = np.array(all_probs)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    # Binary predictions at threshold 0.5\n",
    "    preds = (all_probs >= 0.2609).astype(int)\n",
    "\n",
    "    # Metrics\n",
    "    acc = accuracy_score(all_labels, preds)\n",
    "    auc = roc_auc_score(all_labels, all_probs)\n",
    "    f1 = f1_score(all_labels, preds)\n",
    "    precision = precision_score(all_labels, preds)\n",
    "    recall = recall_score(all_labels, preds)\n",
    "    cm = confusion_matrix(all_labels, preds)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Accuracy:  {acc:.4f}\")\n",
    "    print(f\"AUC:       {auc:.4f}\")\n",
    "    print(f\"F1 Score:  {f1:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    return all_probs, all_labels, preds, cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba439d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs, labels, preds, cm = evaluate_model_full(model, val_loader, device=device)\n",
    "\n",
    "print(\"Probabilities:\", probs[:10])\n",
    "print(\"Predictions:  \", preds[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ffd326",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_ids = []\n",
    "\n",
    "for idx in range(len(probs)):\n",
    "    img_id = dataset.get_img_id(idx)\n",
    "    img_ids.append(img_id)\n",
    "df = pd.DataFrame({\n",
    "    'id': img_ids,\n",
    "    'label': probs\n",
    "})\n",
    "\n",
    "df.to_csv('predictions.csv', index=False, sep='\\t') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509c613a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(labels, probs)\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.plot(fpr, tpr, label=f\"ROC-AUC\")\n",
    "plt.plot([0,1], [0,1], 'k--')  # random line\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate (Recall)\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "J = tpr - fpr\n",
    "ix = np.argmax(J)\n",
    "best_thresh = thresholds[ix]\n",
    "print(\"Best threshold (Youden's J):\", best_thresh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774b6d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from torchvision import transforms\n",
    "\n",
    "# -----------------------\n",
    "# Utilities\n",
    "# -----------------------\n",
    "def normalize01(x):\n",
    "    x = x - x.min()\n",
    "    if x.max() != 0:\n",
    "        x = x / x.max()\n",
    "    return x\n",
    "\n",
    "def pil_from_tensor(tensor):\n",
    "    return transforms.ToPILImage()(tensor.cpu().clamp(0,1))\n",
    "\n",
    "# -----------------------\n",
    "# Grad-CAM generator for ViT (by gradients on ViT patch embeddings)\n",
    "# -----------------------\n",
    "def generate_vit_gradcam(model, input_ids, attention_mask, image_tensor, target_class=None, device='cuda'):\n",
    "    \"\"\"\n",
    "    returns: cam_map (H,W normalized 0..1), predicted_class, confidence, attn_t2i, attn_i2t\n",
    "    input_ids, attention_mask: 1D tensors (for single sample)\n",
    "    image_tensor: 3xHxW tensor (single sample)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # move inputs\n",
    "    input_ids = input_ids.unsqueeze(0).to(device)          # (1, L)\n",
    "    attention_mask = attention_mask.unsqueeze(0).to(device)\n",
    "    image_tensor = image_tensor.unsqueeze(0).to(device)    # (1,3,H,W)\n",
    "\n",
    "    # --- 1) get text features (no grad needed unless you want grads w.r.t text) ---\n",
    "    with torch.no_grad():\n",
    "        text_outputs = model.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_feats = text_outputs.last_hidden_state   # (1, L, D)\n",
    "\n",
    "    # --- 2) get image features but keep gradients on them ---\n",
    "    vit_outputs = model.image_encoder(pixel_values=image_tensor)\n",
    "    img_feats = vit_outputs.last_hidden_state   # (1, P+1, D)\n",
    "    # make img_feats require grad so gradients flow here\n",
    "    img_feats = img_feats.detach()\n",
    "    img_feats.requires_grad_(True)\n",
    "\n",
    "    # --- 3) run symmetric cross-attention manually using model blocks ---\n",
    "    # text_to_img: q=text, k=v=img\n",
    "    # img_to_text: q=img, k=v=text\n",
    "    # We expect these blocks to return (out, attn_weights)\n",
    "    text_cross, attn_t2i = model.text_to_img(text_feats, img_feats, img_feats)\n",
    "    img_cross,  attn_i2t = model.img_to_text(img_feats, text_cross, text_cross, key_padding_mask=(attention_mask==0))  # note: feed updated text_cross for consistency\n",
    "\n",
    "    # --- 4) pool cross-attended representations ---\n",
    "    # per your request — use only cross-attended outputs (mean pooling)\n",
    "    text_emb = text_cross.mean(dim=1)   # (1, D)\n",
    "    img_emb  = img_cross.mean(dim=1)    # (1, D)\n",
    "\n",
    "    # --- 5) classification logits (use cross-attended text_emb per design) ---\n",
    "    logits = model.classifier(text_emb)   # (1, num_classes)\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    pred_class = int(probs.argmax(dim=1).item())\n",
    "    confidence = float(probs[0, pred_class].item())\n",
    "    if target_class is None:\n",
    "        target_class = pred_class\n",
    "\n",
    "    # --- 6) contrastive projections exist in model but not needed here\n",
    "    # compute score and backward to get gradients w.r.t img_feats\n",
    "    score = logits[0, target_class]\n",
    "    model.zero_grad()\n",
    "    score.backward(retain_graph=True)\n",
    "\n",
    "    # --- 7) obtain gradients on img_feats ---\n",
    "    if img_feats.grad is None:\n",
    "        # fallback: try to get gradient from img_cross if not propagated to img_feats\n",
    "        grads = img_cross.grad\n",
    "    else:\n",
    "        grads = img_feats.grad    # (1, P+1, D)\n",
    "\n",
    "    if grads is None:\n",
    "        raise RuntimeError(\"Gradients on image features are None — ensure computation graph connects logits to img_feats.\")\n",
    "\n",
    "    # --- 8) compute patch importance: global-average over feature dim D -->\n",
    "    # grads: (1, P+1, D)  -> avg over D -> (1, P+1)\n",
    "    weights = grads.mean(dim=-1).squeeze(0).detach().cpu().numpy()   # (P+1,)\n",
    "\n",
    "    # ignore CLS token (index 0)\n",
    "    patch_weights = weights[1:]   # (P,)\n",
    "    patch_weights = np.maximum(patch_weights, 0)\n",
    "    patch_weights = normalize01(patch_weights)\n",
    "\n",
    "    # --- 9) reshape to grid ---\n",
    "    # determine grid size from number of patches\n",
    "    P = patch_weights.shape[0]\n",
    "    grid_size = int(np.sqrt(P))\n",
    "    if grid_size * grid_size != P:\n",
    "        # try to infer from vit config if available\n",
    "        try:\n",
    "            img_size = model.image_encoder.config.image_size\n",
    "            patch_size = model.image_encoder.config.patch_size\n",
    "            grid_size = img_size // patch_size\n",
    "        except Exception:\n",
    "            raise ValueError(f\"Cannot infer grid size for P={P}\")\n",
    "    cam_map = patch_weights.reshape(grid_size, grid_size)   # (g,g)\n",
    "    # upscale to image size (assume image_tensor size)\n",
    "    H = image_tensor.shape[-2]\n",
    "    W = image_tensor.shape[-1]\n",
    "    cam_up = cv2.resize(cam_map, (W, H))\n",
    "    cam_up = normalize01(cam_up)\n",
    "\n",
    "    return cam_up, pred_class, confidence, attn_t2i, attn_i2t, text_feats, img_feats.detach()\n",
    "\n",
    "# -----------------------\n",
    "# Attention overlay from text->image: map token -> per-patch scores\n",
    "# -----------------------\n",
    "def attention_patch_scores(attn_t2i, token_index=0):\n",
    "    \"\"\"\n",
    "    attn_t2i may be:\n",
    "      - tensor shape (1, heads, tgt_len_text, src_len_patches)  OR\n",
    "      - tensor shape (1, tgt_len_text, src_len_patches)\n",
    "      - or list/tuple per layer (handle common cases)\n",
    "    We aggregate to get per-patch scores for a chosen token_index.\n",
    "    Returns normalized per-patch vector length P.\n",
    "    \"\"\"\n",
    "    # convert to tensor and squeeze batch\n",
    "    if isinstance(attn_t2i, list) or isinstance(attn_t2i, tuple):\n",
    "        # if list per layer, stack and mean\n",
    "        attn = torch.stack([a for a in attn_t2i], dim=0)   # (L, B, ...)\n",
    "    else:\n",
    "        attn = attn_t2i\n",
    "\n",
    "    attn = torch.as_tensor(attn)\n",
    "    attn = attn.squeeze(0)   # drop batch -> shape may be (heads, T, P) or (T, P)\n",
    "    if attn.ndim == 3:\n",
    "        # (heads, T, P) -> mean over heads -> (T,P)\n",
    "        attn2 = attn.mean(dim=0)\n",
    "    elif attn.ndim == 2:\n",
    "        attn2 = attn\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected attn shape after squeeze: {attn.shape}\")\n",
    "\n",
    "    # attn2: (T, P)\n",
    "    if token_index is None:\n",
    "        # aggregate over tokens\n",
    "        patch_scores = attn2.mean(dim=0).detach().cpu().numpy()\n",
    "    else:\n",
    "        patch_scores = attn2[token_index].detach().cpu().numpy()\n",
    "\n",
    "    patch_scores = np.maximum(patch_scores, 0)\n",
    "    patch_scores = normalize01(patch_scores)\n",
    "    return patch_scores  # (P,)\n",
    "\n",
    "# -----------------------\n",
    "# Visualization: combine Grad-CAM and attention overlay, show top tokens\n",
    "# -----------------------\n",
    "def visualize_multimodal_explanation(model, tokenizer, sample, device='cuda',\n",
    "                                     top_k_tokens=8, token_index=None, patch_size=16, img_size=224):\n",
    "    \"\"\"\n",
    "    sample: (input_ids, attention_mask, image_tensor, label) from your Dataset\n",
    "    token_index: if None -> show aggregated attention over tokens; else integer token index to visualize mapping\n",
    "    \"\"\"\n",
    "    input_ids, attention_mask, image_tensor, label = sample\n",
    "    # run generator\n",
    "    cam_map, pred_class, conf, attn_t2i, attn_i2t, text_feats, img_feats = generate_vit_gradcam(\n",
    "        model, input_ids, attention_mask, image_tensor, device=device\n",
    "    )\n",
    "\n",
    "    # attention patch scores (from text->image) for the token_index\n",
    "    # if token_index is None => aggregate over tokens\n",
    "    # tokenizer.convert_ids_to_tokens for readable tokens\n",
    "    token_ids = input_ids.cpu().numpy()\n",
    "    tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "\n",
    "    # choose token_index: if None -> we'll find top tokens by aggregated attention image->text\n",
    "    # compute image->text aggregated importance (which tokens receive more attention from patches)\n",
    "    # attn_i2t shape handling similar to attn_t2i\n",
    "    # aggregate attn_i2t to per-token importance:\n",
    "    attn_i2t_tensor = attn_i2t\n",
    "    if isinstance(attn_i2t_tensor, list) or isinstance(attn_i2t_tensor, tuple):\n",
    "        attn_i2t_tensor = torch.stack(attn_i2t_tensor, dim=0)\n",
    "    attn_i2t_tensor = torch.as_tensor(attn_i2t_tensor).squeeze(0)  # (heads, P, T) or (P,T)\n",
    "\n",
    "    if attn_i2t_tensor.ndim == 3:\n",
    "        attn_i2t_mean = attn_i2t_tensor.mean(dim=0)   # (P,T)\n",
    "    else:\n",
    "        attn_i2t_mean = attn_i2t_tensor  # (P,T)\n",
    "\n",
    "    # per-token importance: average over patches\n",
    "    token_importance = attn_i2t_mean.mean(dim=0).detach().cpu().numpy()  # (T,)\n",
    "\n",
    "    # top tokens\n",
    "    top_idx = np.argsort(token_importance)[-top_k_tokens:][::-1]\n",
    "    print(\"Top tokens by image->text attention (token, score):\")\n",
    "    for idx in top_idx:\n",
    "        tok = tokens[idx] if idx < len(tokens) else f\"tok{idx}\"\n",
    "        print(f\"  {idx:3d}: {tok:15s}  {token_importance[idx]:.4f}\")\n",
    "\n",
    "    # If token_index is None, we visualize aggregated attention map (mean over tokens)\n",
    "    if token_index is None:\n",
    "        # aggregated text->image attn: get patch scores aggregated over tokens\n",
    "        patch_scores = attention_patch_scores(attn_t2i, token_index=None)  # (P,)\n",
    "    else:\n",
    "        patch_scores = attention_patch_scores(attn_t2i, token_index=token_index)  # (P,)\n",
    "\n",
    "    # reshape to grid\n",
    "    P = patch_scores.shape[0]\n",
    "    patch_scores = patch_scores[1:]\n",
    "    grid = int(np.sqrt(P))\n",
    "    patch_map = patch_scores.reshape(grid, grid)\n",
    "    patch_map_up = cv2.resize(patch_map, (img_size, img_size))\n",
    "    patch_map_up = normalize01(patch_map_up)\n",
    "\n",
    "    # combine cam_map and patch_map_up: show side-by-side and blended\n",
    "    pil_img = pil_from_tensor(image_tensor)\n",
    "    img_np = np.array(pil_img)\n",
    "\n",
    "    # gradcam overlay\n",
    "    heatmap_cam = cv2.applyColorMap(np.uint8(255 * cam_map), cv2.COLORMAP_JET)\n",
    "    heatmap_cam = cv2.cvtColor(heatmap_cam, cv2.COLOR_BGR2RGB)\n",
    "    overlay_cam = (0.6 * img_np + 0.4 * heatmap_cam).astype(np.uint8)\n",
    "\n",
    "    # attention overlay\n",
    "    heatmap_attn = cv2.applyColorMap(np.uint8(255 * patch_map_up), cv2.COLORMAP_JET)\n",
    "    heatmap_attn = cv2.cvtColor(heatmap_attn, cv2.COLOR_BGR2RGB)\n",
    "    overlay_attn = (0.6 * img_np + 0.4 * heatmap_attn).astype(np.uint8)\n",
    "\n",
    "    # blended combination\n",
    "    blended = (0.5 * overlay_cam.astype(float) + 0.5 * overlay_attn.astype(float)).astype(np.uint8)\n",
    "\n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(18, 6))\n",
    "    axes[0].imshow(img_np); axes[0].set_title(\"Original\"); axes[0].axis('off')\n",
    "    axes[1].imshow(overlay_cam); axes[1].set_title(f\"Grad-CAM (pred {pred_class}, conf {conf:.2f})\"); axes[1].axis('off')\n",
    "    axes[2].imshow(overlay_attn); axes[2].set_title(\"Text->Image attention overlay\"); axes[2].axis('off')\n",
    "    axes[3].imshow(blended); axes[3].set_title(\"Blended CAM + Attention\"); axes[3].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # return maps if needed\n",
    "    return {\n",
    "        \"cam_map\": cam_map,\n",
    "        \"patch_map\": patch_map_up,\n",
    "        \"pred_class\": pred_class,\n",
    "        \"confidence\": conf,\n",
    "        \"top_tokens\": [(int(i), tokens[int(i)], float(token_importance[int(i)])) for i in top_idx]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b418a2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = val_set[8199]  # (input_ids, attention_mask, image, label)\n",
    "input_ids, attention_mask, image, label = sample\n",
    "decoded_text = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "print(decoded_text)\n",
    "\n",
    "res = visualize_multimodal_explanation(model, tokenizer, sample, device=device, top_k_tokens=8, token_index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2fe884",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_text import LimeTextExplainer\n",
    "import torch\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "explainer = LimeTextExplainer(class_names=['real', 'fake'])\n",
    "\n",
    "def lime_explain_text(sample_text, image_tensor, model, tokenizer, device='cuda', batch_size=8):\n",
    "    \"\"\"\n",
    "    Explain text contribution in a multimodal model (text + fixed image).\n",
    "    GPU-safe version with batching and cache clearing.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Prepare image\n",
    "    image_tensor = image_tensor.unsqueeze(0).to(device)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def wrapped_predict(text_list):\n",
    "        \"\"\"\n",
    "        LIME will call this many times.\n",
    "        We run it in mini-batches to avoid CUDA OOM.\n",
    "        \"\"\"\n",
    "        probs_all = []\n",
    "\n",
    "        for i in range(0, len(text_list), batch_size):\n",
    "            batch_texts = text_list[i:i + batch_size]\n",
    "\n",
    "            encoding = tokenizer(\n",
    "                batch_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=128,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            input_ids = encoding['input_ids'].to(device, non_blocking=True)\n",
    "            attention_mask = encoding['attention_mask'].to(device, non_blocking=True)\n",
    "\n",
    "            # Repeat image for each text\n",
    "            images = image_tensor.repeat(len(batch_texts), 1, 1, 1)\n",
    "\n",
    "            # Forward\n",
    "            outputs = model(input_ids, attention_mask, images)\n",
    "            logits = outputs[0] if isinstance(outputs, tuple) else outputs\n",
    "            probs = torch.softmax(logits, dim=1).detach().cpu().numpy()\n",
    "            probs_all.append(probs)\n",
    "\n",
    "            # Free CUDA memory after each batch\n",
    "            del input_ids, attention_mask, images, outputs, logits\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        return np.concatenate(probs_all, axis=0)\n",
    "\n",
    "    # LIME explanation (this still runs on CPU)\n",
    "    exp = explainer.explain_instance(\n",
    "        sample_text,\n",
    "        wrapped_predict,\n",
    "        num_features=10,\n",
    "        labels=(0, 1)\n",
    "    )\n",
    "\n",
    "    return exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffaabe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7448bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = val_set[8199]  # (input_ids, attention_mask, image, label)\n",
    "input_ids, attention_mask, image, label = sample\n",
    "decoded_text = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "explanation = lime_explain_text(decoded_text, image, model, tokenizer)\n",
    "print(explanation)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac8cd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation.as_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a23e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation.as_html(predict_proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f09808",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"explanation2.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(explanation.as_html())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2356dc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime import lime_image\n",
    "from skimage.segmentation import slic\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# -------------------------\n",
    "# Helper: try to find a good target layer in ViT encoder\n",
    "# -------------------------\n",
    "def reshape_transform(tensor, height=14, width=14):\n",
    "    # Remove CLS token\n",
    "    result = tensor[:, 1:, :]  \n",
    "    # Reshape to (B, H, W, C)\n",
    "    result = result.reshape(result.size(0), height, width, -1)\n",
    "    # Permute to (B, C, H, W)\n",
    "    result = result.permute(0, 3, 1, 2)\n",
    "    return result\n",
    "# -------------------------\n",
    "# Wrapper model for GradCAM\n",
    "# -------------------------\n",
    "class MultiModalForCAM(nn.Module):\n",
    "    \"\"\"\n",
    "    A wrapper that accepts only images as input and returns logits,\n",
    "    it uses a fixed text (input_ids, attention_mask).\n",
    "    This allows grad-cam to get gradients across image_encoder layers,\n",
    "    but consider the target relative to the final logit of the multi-modal model.\n",
    "    \"\"\"\n",
    "    def __init__(self, full_model, fixed_input_ids, fixed_attention_mask):\n",
    "        super().__init__()\n",
    "        self.full_model = full_model\n",
    "        # make sure fixed inputs are detached and on the correct device when used\n",
    "        self.register_buffer('fixed_input_ids', fixed_input_ids.squeeze(0))\n",
    "        self.register_buffer('fixed_attention_mask', fixed_attention_mask.squeeze(0))\n",
    "\n",
    "    def forward(self, images):\n",
    "        # images: tensor (B, C, H, W)\n",
    "        # repeat fixed text for batch\n",
    "        batch_size = images.shape[0]\n",
    "        input_ids = self.fixed_input_ids.unsqueeze(0).repeat(batch_size, 1)\n",
    "        attention_mask = self.fixed_attention_mask.unsqueeze(0).repeat(batch_size, 1)\n",
    "        logits, *_ = self.full_model(input_ids=input_ids, attention_mask=attention_mask, images=images)\n",
    "        return logits\n",
    "\n",
    "# -------------------------\n",
    "# LIME predict wrapper for images (multimodal)\n",
    "# -------------------------\n",
    "def make_lime_predict_fn(model, input_ids, attention_mask, transform_val, device):\n",
    "    \"\"\"\n",
    "    returns func pred_fn(images_np) -> probs_numpy (N x num_classes).\n",
    "    images_np: array (N, H, W, 3), dtype uint8 (0..255).\n",
    "    uses transform_val (PIL->tensor->normalize) and fixed text.\n",
    "    \"\"\"\n",
    "    def pred_fn(images_np):\n",
    "        model.eval()\n",
    "        tensors = []\n",
    "        for im in images_np:\n",
    "            # im: HWC, uint8\n",
    "            pil = Image.fromarray(im.astype('uint8'), 'RGB')\n",
    "            x = transform_val(pil).unsqueeze(0)  # 1, C, H, W\n",
    "            tensors.append(x)\n",
    "        batch = torch.cat(tensors, dim=0).to(device)  # (N, C, H, W)\n",
    "        # prepare text inputs (repeat)\n",
    "        b = batch.shape[0]\n",
    "        ids = input_ids.unsqueeze(0).repeat(b,1).to(device)\n",
    "        mask = attention_mask.unsqueeze(0).repeat(b,1).to(device)\n",
    "        with torch.no_grad():\n",
    "            logits, *_ = model(ids, mask, batch)\n",
    "            probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "        return probs\n",
    "    return pred_fn\n",
    "\n",
    "# -------------------------\n",
    "# Main explain function: runs GradCAM + LIME and plots/saves results\n",
    "# -------------------------\n",
    "def explain_image_with_lime_and_gradcam(model, tokenizer,\n",
    "                                        input_ids, attention_mask, image_tensor,\n",
    "                                        transform_val,\n",
    "                                        device='cuda',\n",
    "                                        lime_samples=200,\n",
    "                                        top_label=None,\n",
    "                                        save_path=None):\n",
    "    \"\"\"\n",
    "    input_ids, attention_mask: tensors for the text (1, seq_len) or (seq_len,)\n",
    "    image_tensor: (C, H, W) tensor (unnormalized? assume already normalized as used by model)\n",
    "    transform_val: pipeline used in validation (PIL->Tensor->Normalize) for LIME preprocessing\n",
    "    lime_samples: number of perturbed samples for LIME\n",
    "    top_label: int or None (if None, use predicted label)\n",
    "    save_path: if specified, saves image\n",
    "    returns matplotlib.Figure\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    image = image_tensor.clone().detach().to(device)\n",
    "    # prepare original image in HWC 0..1 for show_cam_on_image\n",
    "    # undo normalization if transform_val uses Normalize with ImageNet mean/std\n",
    "    # We'll try to reconstruct 0..1 image from normalized tensor:\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std  = np.array([0.229, 0.224, 0.225])\n",
    "    im_np = image.cpu().numpy().transpose(1,2,0)  # HWC but normalized\n",
    "    im_01 = (im_np * std + mean).clip(0,1)  # approximate original normalized to 0..1\n",
    "\n",
    "    # --- GradCAM ---\n",
    "    # determine a target layer inside ViT\n",
    "    # target_layer = get_vit_target_layer(model.image_encoder)\n",
    "    target_layer = model.image_encoder.encoder.layer[-1].output\n",
    "    # wrap multimodal model for GradCAM\n",
    "    wrapper = MultiModalForCAM(model, input_ids.unsqueeze(0).to(device), attention_mask.unsqueeze(0).to(device))\n",
    "    wrapper.to(device)\n",
    "    # create GradCAM object\n",
    "    cam = GradCAM(model=wrapper, target_layers=[target_layer], reshape_transform=reshape_transform)\n",
    "    # input to CAM must be a tensor (B, C, H, W)\n",
    "    with torch.no_grad():\n",
    "        inp = image.unsqueeze(0).to(device)\n",
    "    # optionally compute target category: top_label or predicted\n",
    "    with torch.no_grad():\n",
    "        logits, *_ = model(input_ids=input_ids.unsqueeze(0).to(device),\n",
    "                           attention_mask=attention_mask.unsqueeze(0).to(device),\n",
    "                           images=inp)\n",
    "        probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "        pred_label = int(probs.argmax(axis=1)[0])\n",
    "    target_category = pred_label if top_label is None else int(top_label)\n",
    "\n",
    "    grayscale_cam = cam(input_tensor=inp, targets=None)[0]  # HxW\n",
    "    cam_image = show_cam_on_image(im_01, grayscale_cam, use_rgb=True)  # uint8 HxW3\n",
    "\n",
    "    # --- LIME (image) ---\n",
    "    explainer = lime_image.LimeImageExplainer()\n",
    "    predict_fn = make_lime_predict_fn(model, input_ids, attention_mask, transform_val, device)\n",
    "    # lime wants HWC uint8 0..255\n",
    "    im_for_lime = (im_01 * 255).astype('uint8')\n",
    "    # run explain_instance (this is relatively heavy)\n",
    "    explanation = explainer.explain_instance(\n",
    "        im_for_lime,\n",
    "        classifier_fn=predict_fn,\n",
    "        top_labels=2,\n",
    "        hide_color=0,\n",
    "        num_samples=lime_samples,\n",
    "        segmentation_fn=lambda x: slic(x, n_segments=50, compactness=10)  # faster/better segmentation\n",
    "    )\n",
    "    # choose label to visualize\n",
    "    label_to_vis = target_category\n",
    "    temp, mask = explanation.get_image_and_mask(label_to_vis,\n",
    "                                                positive_only=False,\n",
    "                                                num_features=10,\n",
    "                                                hide_rest=False)\n",
    "    # temp: HxWx3 uint8 with highlighted superpixels\n",
    "    lime_overlay = temp\n",
    "\n",
    "    # --- plot side-by-side ---\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15,5))\n",
    "    axes[0].imshow((im_01.clip(0,1)))\n",
    "    axes[0].set_title('Original (approx)')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    axes[1].imshow(cam_image)\n",
    "    axes[1].set_title(f'Grad-CAM (pred={pred_label})')\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    axes[2].imshow(lime_overlay)\n",
    "    axes[2].set_title('LIME overlay')\n",
    "    axes[2].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        fig.savefig(save_path, bbox_inches='tight', dpi=200)\n",
    "    return fig, {'pred_prob': probs[0].tolist(), 'pred_label': pred_label, 'grayscale_cam': grayscale_cam, 'lime_mask': mask}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7855c58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, meta = explain_image_with_lime_and_gradcam(model, tokenizer,\n",
    "                                                input_ids, attention_mask, image,\n",
    "                                                transform_val, device=device,\n",
    "                                                lime_samples=300,\n",
    "                                                save_path='explanations/sample0.png')\n",
    "\n",
    "plt.show(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
