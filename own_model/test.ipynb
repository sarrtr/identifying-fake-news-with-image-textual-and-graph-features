{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0ebe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.optim import AdamW\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel, ViTModel\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c93b4b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cross Attention ---\n",
    "class CrossAttentionBlock(nn.Module):\n",
    "    def __init__(self, dim_q, dim_k, num_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=dim_q, num_heads=num_heads, batch_first=True)\n",
    "        self.norm_q = nn.LayerNorm(dim_q)\n",
    "        self.norm_k = nn.LayerNorm(dim_k)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.proj = nn.Linear(dim_q, dim_q)\n",
    "\n",
    "    def forward(self, q, k, v, key_padding_mask=None):\n",
    "        q_norm, k_norm = self.norm_q(q), self.norm_k(k)\n",
    "        attn_out, attn_weights = self.attn(\n",
    "            q_norm, k_norm, v, key_padding_mask=key_padding_mask, need_weights=True\n",
    "        )\n",
    "        q_out = q + self.dropout(self.proj(attn_out))\n",
    "        return q_out, attn_weights\n",
    "\n",
    "\n",
    "# --- Symmetric Multimodal Classifier ---\n",
    "class SymmetricMultimodalClassifier(nn.Module):\n",
    "    def __init__(self, \n",
    "                 text_model='bert-base-uncased', \n",
    "                 image_model='google/vit-base-patch16-224',\n",
    "                 hidden_dim=768, \n",
    "                 num_classes=2, \n",
    "                 use_contrastive=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.text_encoder = BertModel.from_pretrained(text_model)\n",
    "        self.image_encoder = ViTModel.from_pretrained(image_model)\n",
    "        self.use_contrastive = use_contrastive\n",
    "\n",
    "        self.text_to_img = nn.Sequential(\n",
    "            CrossAttentionBlock(hidden_dim, hidden_dim),\n",
    "            CrossAttentionBlock(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        self.img_to_text = nn.Sequential(\n",
    "            CrossAttentionBlock(hidden_dim, hidden_dim),\n",
    "            CrossAttentionBlock(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "        self.text_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.img_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "        # Для Grad-CAM\n",
    "        self.last_attn_t2i = None\n",
    "        self.last_attn_i2t = None\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, images):\n",
    "        # --- Encode text & image ---\n",
    "        text_feat = self.text_encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=False\n",
    "        ).last_hidden_state  # [B, L_t, D]\n",
    "\n",
    "        img_feat = self.image_encoder(images).last_hidden_state  # [B, L_i, D]\n",
    "        key_padding_mask = (attention_mask == 0)\n",
    "\n",
    "        # --- Symmetric Cross-Attention ---\n",
    "        text_cross, attn_t2i = self.text_to_img(text_feat, img_feat, img_feat)\n",
    "        img_cross, attn_i2t = self.img_to_text(img_feat, text_feat, text_feat, key_padding_mask)\n",
    "\n",
    "        # Save for Grad-CAM visualization\n",
    "        self.last_attn_t2i = attn_t2i.detach()\n",
    "        self.last_attn_i2t = attn_i2t.detach()\n",
    "\n",
    "        # --- Use CLS tokens only ---\n",
    "        # text_emb = text_cross[:, 0, :]  # CLS\n",
    "        text_emb = text_cross[:,0]\n",
    "        img_emb = img_cross[:, 0, :]    # CLS\n",
    "        \n",
    "\n",
    "        # --- Normalize for contrastive loss ---\n",
    "        text_emb_n = F.normalize(self.text_proj(text_emb), p=2, dim=-1)\n",
    "        img_emb_n = F.normalize(self.img_proj(img_emb), p=2, dim=-1)\n",
    "\n",
    "        # --- Classification ---\n",
    "        logits = self.classifier(text_emb)\n",
    "\n",
    "        if self.use_contrastive:\n",
    "            return logits, text_emb_n, img_emb_n, attn_t2i, attn_i2t\n",
    "        else:\n",
    "            return logits, attn_t2i, attn_i2t\n",
    "\n",
    "\n",
    "# --- Contrastive Loss (Symmetric) ---\n",
    "def contrastive_loss(text_emb, img_emb, temperature=0.05):\n",
    "    sim_matrix = torch.matmul(text_emb, img_emb.T) / temperature\n",
    "    labels = torch.arange(sim_matrix.size(0), device=sim_matrix.device)\n",
    "    loss_i = F.cross_entropy(sim_matrix, labels)\n",
    "    loss_t = F.cross_entropy(sim_matrix.T, labels)\n",
    "    return (loss_i + loss_t) / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "43fc94f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalNewsDataset(Dataset):\n",
    "    def __init__(self, txt_file, image_dir, tokenizer, max_len=128, transform=None):\n",
    "        self.data = pd.read_csv(txt_file, sep='\\t')\n",
    "        self.image_dir = image_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        text = str(row['clean_title'])\n",
    "        img_id = str(row['id'])\n",
    "        label = torch.tensor(int(row['2_way_label']), dtype=torch.long)\n",
    "        \n",
    "        img_path = os.path.join(self.image_dir, f\"{img_id}.jpg\")\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            # print(f\"[WARN] Ошибка при открытии {img_path}: {e}\")\n",
    "            # Возвращаем пустое изображение, чтобы не ломать batch\n",
    "            image = Image.new(\"RGB\", (224, 224), (0, 0, 0))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            image = transforms.ToTensor()(image)\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].squeeze(0)\n",
    "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
    "        \n",
    "        return input_ids, attention_mask, image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "bc108786",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "dataset_path = '/repo/project_deepfake/project/fakeddit_dataset'\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dataset = MultimodalNewsDataset(\n",
    "    txt_file=dataset_path+'/text/text.txt',\n",
    "    image_dir=dataset_path+'/images',\n",
    "    tokenizer=tokenizer,\n",
    "    transform=transform_train \n",
    ")\n",
    "\n",
    "# Split indices\n",
    "train_idx, val_idx = train_test_split(\n",
    "    range(len(dataset)),\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=dataset.data['2_way_label']\n",
    ")\n",
    "\n",
    "train_set = Subset(dataset, train_idx)\n",
    "val_set   = Subset(dataset, val_idx)\n",
    "\n",
    "val_set.dataset.transform = transform_val  \n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f1793f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for SymmetricMultimodalClassifier:\n\tMissing key(s) in state_dict: \"text_to_img.0.attn.in_proj_weight\", \"text_to_img.0.attn.in_proj_bias\", \"text_to_img.0.attn.out_proj.weight\", \"text_to_img.0.attn.out_proj.bias\", \"text_to_img.0.norm_q.weight\", \"text_to_img.0.norm_q.bias\", \"text_to_img.0.norm_k.weight\", \"text_to_img.0.norm_k.bias\", \"text_to_img.0.proj.weight\", \"text_to_img.0.proj.bias\", \"text_to_img.1.attn.in_proj_weight\", \"text_to_img.1.attn.in_proj_bias\", \"text_to_img.1.attn.out_proj.weight\", \"text_to_img.1.attn.out_proj.bias\", \"text_to_img.1.norm_q.weight\", \"text_to_img.1.norm_q.bias\", \"text_to_img.1.norm_k.weight\", \"text_to_img.1.norm_k.bias\", \"text_to_img.1.proj.weight\", \"text_to_img.1.proj.bias\", \"img_to_text.0.attn.in_proj_weight\", \"img_to_text.0.attn.in_proj_bias\", \"img_to_text.0.attn.out_proj.weight\", \"img_to_text.0.attn.out_proj.bias\", \"img_to_text.0.norm_q.weight\", \"img_to_text.0.norm_q.bias\", \"img_to_text.0.norm_k.weight\", \"img_to_text.0.norm_k.bias\", \"img_to_text.0.proj.weight\", \"img_to_text.0.proj.bias\", \"img_to_text.1.attn.in_proj_weight\", \"img_to_text.1.attn.in_proj_bias\", \"img_to_text.1.attn.out_proj.weight\", \"img_to_text.1.attn.out_proj.bias\", \"img_to_text.1.norm_q.weight\", \"img_to_text.1.norm_q.bias\", \"img_to_text.1.norm_k.weight\", \"img_to_text.1.norm_k.bias\", \"img_to_text.1.proj.weight\", \"img_to_text.1.proj.bias\". \n\tUnexpected key(s) in state_dict: \"text_pool.proj.weight\", \"text_pool.proj.bias\", \"img_pool.proj.weight\", \"img_pool.proj.bias\", \"text_to_img.attn.in_proj_weight\", \"text_to_img.attn.in_proj_bias\", \"text_to_img.attn.out_proj.weight\", \"text_to_img.attn.out_proj.bias\", \"text_to_img.norm_q.weight\", \"text_to_img.norm_q.bias\", \"text_to_img.norm_k.weight\", \"text_to_img.norm_k.bias\", \"img_to_text.attn.in_proj_weight\", \"img_to_text.attn.in_proj_bias\", \"img_to_text.attn.out_proj.weight\", \"img_to_text.attn.out_proj.bias\", \"img_to_text.norm_q.weight\", \"img_to_text.norm_q.bias\", \"img_to_text.norm_k.weight\", \"img_to_text.norm_k.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[147], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoints/multimodal_model.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m SymmetricMultimodalClassifier()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_state_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/lib/python3.12/site-packages/torch/nn/modules/module.py:2189\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2184\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2185\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2186\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2190\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for SymmetricMultimodalClassifier:\n\tMissing key(s) in state_dict: \"text_to_img.0.attn.in_proj_weight\", \"text_to_img.0.attn.in_proj_bias\", \"text_to_img.0.attn.out_proj.weight\", \"text_to_img.0.attn.out_proj.bias\", \"text_to_img.0.norm_q.weight\", \"text_to_img.0.norm_q.bias\", \"text_to_img.0.norm_k.weight\", \"text_to_img.0.norm_k.bias\", \"text_to_img.0.proj.weight\", \"text_to_img.0.proj.bias\", \"text_to_img.1.attn.in_proj_weight\", \"text_to_img.1.attn.in_proj_bias\", \"text_to_img.1.attn.out_proj.weight\", \"text_to_img.1.attn.out_proj.bias\", \"text_to_img.1.norm_q.weight\", \"text_to_img.1.norm_q.bias\", \"text_to_img.1.norm_k.weight\", \"text_to_img.1.norm_k.bias\", \"text_to_img.1.proj.weight\", \"text_to_img.1.proj.bias\", \"img_to_text.0.attn.in_proj_weight\", \"img_to_text.0.attn.in_proj_bias\", \"img_to_text.0.attn.out_proj.weight\", \"img_to_text.0.attn.out_proj.bias\", \"img_to_text.0.norm_q.weight\", \"img_to_text.0.norm_q.bias\", \"img_to_text.0.norm_k.weight\", \"img_to_text.0.norm_k.bias\", \"img_to_text.0.proj.weight\", \"img_to_text.0.proj.bias\", \"img_to_text.1.attn.in_proj_weight\", \"img_to_text.1.attn.in_proj_bias\", \"img_to_text.1.attn.out_proj.weight\", \"img_to_text.1.attn.out_proj.bias\", \"img_to_text.1.norm_q.weight\", \"img_to_text.1.norm_q.bias\", \"img_to_text.1.norm_k.weight\", \"img_to_text.1.norm_k.bias\", \"img_to_text.1.proj.weight\", \"img_to_text.1.proj.bias\". \n\tUnexpected key(s) in state_dict: \"text_pool.proj.weight\", \"text_pool.proj.bias\", \"img_pool.proj.weight\", \"img_pool.proj.bias\", \"text_to_img.attn.in_proj_weight\", \"text_to_img.attn.in_proj_bias\", \"text_to_img.attn.out_proj.weight\", \"text_to_img.attn.out_proj.bias\", \"text_to_img.norm_q.weight\", \"text_to_img.norm_q.bias\", \"text_to_img.norm_k.weight\", \"text_to_img.norm_k.bias\", \"img_to_text.attn.in_proj_weight\", \"img_to_text.attn.in_proj_bias\", \"img_to_text.attn.out_proj.weight\", \"img_to_text.attn.out_proj.bias\", \"img_to_text.norm_q.weight\", \"img_to_text.norm_q.bias\", \"img_to_text.norm_k.weight\", \"img_to_text.norm_k.bias\". "
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(\"checkpoints/multimodal_model.pth\", map_location=device)\n",
    "model = SymmetricMultimodalClassifier().to(device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a1ecf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
